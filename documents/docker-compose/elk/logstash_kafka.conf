input {

  # #读取kafka
  kafka {
    # type => "kafka_1"
    # bootstrap_servers => "localhost:9092" # Kafka 服务器地址和端口
    # decorate_events => true
    # topics => ["DEMO_01"] # Kafka 主题
    # group_id => "emo01-A-consumer-group-DEMO_01" # Kafka 消费者组
    # consumer_threads => 3 # 消费者线程数
    # codec => "json" # 如果日志是JSON格式的
    bootstrap_servers => "124.225.163.92:59092"
    decorate_events => true
    client_id => "logstash-client"
    group_id => "logstash-group"
    topics => ["e1_data_3"]
    #topics_pattern => ".*"  # 使用正则表达式匹配所有topics
    consumer_threads => 3 # 消费者线程数
    codec => "json"
    # SASL/PLAIN认证配置
    security_protocol => "SASL_PLAINTEXT"
    sasl_mechanism => "PLAIN"
    sasl_jaas_config => 'org.apache.kafka.common.security.plain.PlainLoginModule required username="wanji" password="hn4132025!";' # 对于PLAIN机制

  }
}

filter {

  date {
    match => ["counttime", "yyyy-MM-dd HH:mm:ss"]
    target => "@counttime"
  }

  grok {
    match => { "message" => "(?<counttime>^\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2}\.\d{3})\s(?<content>.+)" }
    add_field => { "counttime" => "%{@counttime}" }
    #remove_field => ["message"]
  }

  ruby {
    code => "event.set('indexDay', Date.today.strftime('%Y-%m-%d'))"
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "log-3-%{+YYYY.MM.dd}"
    codec => json
  }
  stdout {
    codec => rubydebug
  }
}
